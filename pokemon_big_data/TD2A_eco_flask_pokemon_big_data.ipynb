{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application en Flask\n",
    "\n",
    "Aujourd'hui nous allons reporduire en partie le jeu \"Big Data ou Pokemon\" disponible à l'adresse suivante https://pixelastic.github.io/pokemonorbigdata/\n",
    "\n",
    "Pour réussir cela, nous aurons besoin de plusieurs choses : \n",
    "- une liste des noms de pokemon\n",
    "- une liste de techologies Big data\n",
    "- un moyen de créer un site internet avec Python (et c'est là que Flask apparait)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupérer la liste des noms des pokemons\n",
    "\n",
    "L'exercice de scraping du TD \"WebScraping\" nous donne leur noms ainsi qu'une rapide description des pokemons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bulbasaur', 'Ivysaur', 'Venusaur', 'Charmander', 'Charmeleon', 'Charizard', 'Squirtle', 'Wartortle', 'Blastoise', 'Caterpie']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "db_pokemon = pd.read_csv(\"./pokemons_database.csv\", encoding=\"utf-8\" )\n",
    "dict_pokemons = db_pokemon[['name','desc']].set_index('name').to_dict()['desc']\n",
    "print(list(dict_pokemons.keys())[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupérer la liste des technologies Big Data\n",
    "\n",
    "Pour cela, nous allons passer par le site http://usefulstuff.io/big-data/ qui donne une liste assez complète de ce qui existe sur le marché"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno -2] Name or service not known>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:1354\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1354\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:1256\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:1302\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:1251\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1251\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:1011\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1011\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1014\u001b[0m \n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:951\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:922\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;124;03m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39msetsockopt(socket\u001b[38;5;241m.\u001b[39mIPPROTO_TCP, socket\u001b[38;5;241m.\u001b[39mTCP_NODELAY, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:787\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    786\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    788\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:918\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    917\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 918\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    919\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m req \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://usefulstuff.io/big-data/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      8\u001b[0m page \u001b[38;5;241m=\u001b[39m bs4\u001b[38;5;241m.\u001b[39mBeautifulSoup(html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    522\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    524\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 525\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    528\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:542\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    541\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 542\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    543\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    501\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 502\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:1383\u001b[0m, in \u001b[0;36mHTTPHandler.http_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:1357\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1355\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1357\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1358\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno -2] Name or service not known>"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import bs4\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "req = urllib.request.Request('http://usefulstuff.io/big-data/')\n",
    "html = urllib.request.urlopen(req).read()\n",
    "page = bs4.BeautifulSoup(html, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_big_data_techno = collections.defaultdict()\n",
    "for body in page.findAll('div', {'class' : 'entry-content'}) : \n",
    "    for part in body.findAll('ul') : \n",
    "        for item_list in body.findAll('li') : \n",
    "            if \":\" in item_list.get_text() : \n",
    "                desc_big_data = item_list.get_text().replace(\"Ê\",'').replace(\"’\",'').replace(\" \",'')\n",
    "                dict_big_data_techno[desc_big_data.split(':')[0]] = desc_big_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voudra supprimer les mots qui font directement référence à une société connue dans le nom du produt (Facebook, Twitter...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Netezza': 'IBMNetezza:high-performancedatawarehouseappliances', 'Accumulo': 'ApacheAccumulo:distribuitedkey/valuestore,builton\\xa0Hadoop', 'Monet': 'MonetDB:columnstoredatabase', 'Proxy': 'ProxySQL:HighPerformanceProxyforMySQL', 'Level': 'LevelDB:afastkey-valuestoragelibrarywrittenatGooglethatprovidesanorderedmappingfromstringkeystostringvalues', 'Cosmos': 'MicrosoftCosmos:MicrosoftsinternalBigDataanalysisplatform', 'Memcache': 'MemcacheDB:adistributedkey-valuestoragesystemdesignedforpersistent', 'Stream': 'MicrosoftAzureStreamAnalytics:aneventprocessingenginethathelpsuncoverreal-timeinsightsfromdevices,sensors,infrastructure,applicationsanddata', 'Pigpen': 'NetflixPigPen:map-reduceforClojurewhichecompilestoApachePig', 'Zalonibedrock': 'ZaloniBedrock:fullyintegratedHadoopdatamanagementplatform', 'Peity': 'Peity:ProgressiveSVGbar,lineandpiecharts', 'Ayasdicore': 'AyasdiCore:toolfortopologicaldataanalysis', 'Squarecubism': 'SquareCubism.js:aD3pluginforvisualizingtimeseries.UseCubismtoconstructbetterrealtimedashboards,pullingdatafromGraphite,Cubeandothersources', 'Minotaur': 'Minotaur:scripts/recipes/configstospinupVPC-basedinfrastructureinAWSfromscratchanddeploylabstoit', 'Keylines': 'Keylines:toolkitforvisualizingthenetworksinyourdata', 'Unicorn': 'FacebookUnicorn:socialgraphsearchplatform', 'Akela': 'Akela:MozillasutilitylibraryforHadoop,HBase,Pig,etc.', 'Openmpi': 'OpenMPI:messagepassingframework', 'Eureka': 'NetflixEureka:AWSServiceregistryforresilientmid-tierloadbalancingandfailover', 'Machinelearning': 'AmazonMachineLearning:visualizationtoolsandwizardsthatguideyouthroughtheprocessofcreatingmachinelearning(ML)modelswithouthavingtolearncomplexMLalgorithmsandtechnology', 'Aerospike': 'Aerospike:NoSQLflash-optimized,in-memory.Opensourceand“Servercodein‘C(notJavaorErlang)preciselytunedtoavoidcontextswitchingandmemorycopies.', 'Bigtable': 'GoogleBigTable:column-orienteddistributeddatastore', 'Envisionjs': 'Envisionjs:dynamicHTML5visualization', 'Discoddfs': 'DiscoDDFS:distributedfilesystem', 'Hekaton': 'Hekaton:Refertolock-freearchitectureforSQLServer2014', 'Galera': 'GaleraCluster:asynchronousmulti-masterclusterforMySQL,PerconaandMariaDB', 'Cockroach': 'Cockroach:Scalable,Geo-Replicated,TransactionalDatastore', 'Arrow': 'ApacheArrow:PoweringColumnarIn-MemoryAnalytics', 'Genie': 'Genie:GenieprovidesREST-fulAPIstorunHadoop,HiveandPigjobs,andtomanagemultipleHadoopresourcesandperformjobsubmissionsacrossthem.', 'Presto': 'FacebookPrestoDB:distributedSQLqueryengine', 'Concurrentcascading': 'ConcurrentCascading:frameworkfordatamanagement/analyticsonHadoop', 'Elasticsearch': 'ElasticSearch:SearchandanalyticsenginebasedonApache\\xa0Lucene', 'Periscope': 'Periscope:plugsdirectlyintoyourdatabasesandletsyourun,save,andshareanalysesoverbillionsofdatarowsinseconds', 'Zillabyte': 'Zillabyte:anAPIfordistributeddatacomputation.Scalewithyourdata.', 'Torch': 'Torch:ScientificcomputingforLuaJIT', 'Ipython': 'IPython:providesaricharchitectureforinteractivecomputing', 'Crane': 'TwitterCrane:JavaETL', 'Atigeoxpatterns': 'AtigeoxPatterns:dataanalyticsplatform', 'Cratedata': 'CrateData:isanopensourcemassivelyscalabledatastore.Itrequireszeroadministration', 'Squarecube': 'SquareCube:systemforcollectingtimestampedeventsandderivingmetrics', 'Marconi': 'Marconi:queuingandnotificationservicemadebyandforOpenStack,butnotonlyforit', 'Neflixsimianarmy': 'NeflixSimianArmy:asuiteoftoolsforkeepingyourcloudoperatingintopform', 'Vertica': 'Vertica:isdesignedtomanagelarge,fast-growingvolumesofdataandprovideveryfastqueryperformancewhenusedfordatawarehouses', 'Brain': 'brain:NeuralnetworksinJavaScript', 'Symmetricds': 'SymmetricDS:opensourcesoftwareforbothfileanddatabasesynchronization', 'Springfor': 'SpringforApacheHadoop:unifiedconfigurationmodelandeasytouseAPIsforusingHDFS,MapReduce,Pig,andHive', 'Rethink': 'RethinkDB:documentdatabasethatsupportsqueriesliketablejoinsandgroupby', 'Mongo': 'MongoDB:Document-orienteddatabasesystem', 'Adatao': 'Adatao:businessintelligenceanddatascienceplatform', 'Spindle': 'AdobeSpindle:Next-generationwebanalyticsprocessingwithScala,Spark,andParquet', 'Ampcrowd': 'AMPcrowd:ARESTfulwebservicethatrunsmicrotasksacrossmultiplecrowds', 'Colossus': 'GoogleColossus:distributedfilesystem(GFS2)', 'Scalding': 'TwitterScalding:ScalalibraryforMapReducejobs,builtonCascading', 'Redshift': 'AmazonRedShift:datawarehouseservice,basedonPostgreSQL', 'Chronix': 'Chronix:fastandefficienttimeseriesstoragebasedonApacheLuceneandApacheSolr', 'Hystrix': 'NetflixHystrix:alatencyandfaulttolerancelibrarydesignedtoisolatepointsofaccesstoremotesystems,servicesand3rdpartylibraries,stopcascadingfailureandenableresilienceincomplexdistributedsystemswherefailureisinevitable', 'Splunk': 'Splunk:analyzerformachine-generateddate', 'Hcatalog': 'ApacheHCatalog:tableandstoragemanagementlayerforHadoop', 'Datawarehouse': 'MicrosoftAzureSQLDataWarehouse:businessesaccesstoanelasticpetabyte-scale,datawarehouse-as-a-serviceofferingthatcanscaleaccordingtotheirneeds', 'Predictionio': 'PredictionIO:machinelearningserverbuitonHadoop,MahoutandCascading', 'Manhattan': 'TwitterManhattan:real-time,multi-tenantdistributeddatabaseforTwitterscale', 'Celery': 'Celery:DistributedTaskQueue', 'Zeppelin': 'ApacheZeppelin:aweb-basednotebookthatenablesinteractivedataanalytics', 'Voldemort': 'LinkedinVoldemort:distributedkey/valuestoragesystem', 'Cortana': 'MicrosoftCortanaAnalytics:afullymanagedbigdataandadvancedanalyticssuitethatenablesyoutotransformyourdataintointelligentaction.', 'Sense': 'Sense:CloudPlatformforDataScienceandBigDataAnalytics', 'Pubnub': 'Pubnub:Datastreamnetwork', 'Tumblrcollins': 'TumblrCollins:Infrastructuremanagementforengineers', 'Tensorflow': 'GoogleTensorFlow:anOpenSourceSoftwareLibraryforMachineIntelligence', 'Scalaris': 'Scalaris:adistributedtransactionalkey-valuestore', 'Secor': 'PinterestSecor:isaserviceimplementingKafkalogpersistance', 'Finagle': 'TwitterFinagle:asynchronousnetworkstackfortheJVM', 'Maria': 'MariaDB:enhanced,drop-inreplacementforMySQL', 'Teradataquerygrid': 'TeradataQueryGrid:data-accesslayerthatcanorchestratemultiplemodesofanalysisacrossmultipledatabasesplusHadoop', 'Storagenearline': 'GoogleCloudStorageNearline:ahighlyavailable,affordablesolutionforbackup,archivinganddisasterrecovery.', 'Kubernetes': 'Kubernetes:opensourceimplementationofcontainerclustermanagement', 'Esper': 'Esper:ahighlyscalable,memory-efficient,in-memorycomputing,SQL-standard,minimallatency,real-timestreaming-capableBigDataprocessingengineforhistoricaldata', 'Gremlin': 'Gremlin:graphtraversalLanguage', 'Tokumx': 'TokuMX:High-PerformanceMongoDBDistribution', 'Perconaserver': 'PerconaServer:enhanced,drop-inreplacementforMySQL', 'Streamdrill': 'Streamdrill:usefullforcountingactivitiesofeventstreamsoverdifferenttimewindowsandfindingthemostactiveone', 'Deeptext': 'FacebookDeepText:adeeplearning-basedtextunderstandingenginethatcanunderstandwithnear-humanaccuracythetextualcontentofseveralthousandspostspersecond,spanningmorethan20languages', 'Chartio': 'Chartio:leanbusinessintelligenceplatformtovisualizeandexploreyourdata', 'Trident': 'Trident:ahigh-levelabstractionfordoingrealtimecomputingontopofStorm', 'Pachyderm': 'Pachyderm:letsyoustoreandanalyzeyourdatausingcontainers.', 'Theano': 'Theano:PythonpackagefordeeplearningthatcanutilizeNVIDIAsCUDAtoolkittorunontheGPU', 'Faunus': 'Faunus:Hadoop-basedgraphanalyticsengineforanalyzinggraphsrepresentedacrossamulti-machinecomputecluster', 'Apamaanalytics': 'Apamaanalytics:platformforstreaminganalyticsandintelligentautomatedaction', 'Doradus': 'Doradus:DoradusisaRESTservicethatextendsaCassandraNoSQLdatabasewithagraph-baseddatamodel,advancedindexingandsearchfeatures,andaRESTAPI', 'Scribe': 'FacebookScribe:streamedlogdataaggregator', 'Hydrabase': 'FacebookHydraBase:evolutionofHBasemadebyFacebook', 'Carto': 'CartoDB:open-sourceorfreemiumhostingforgeospatialdatabaseswithpowerfulfront-endeditingcapabilitiesandarobustAPI', 'Berkeley': 'BerkeleyDB:asoftwarelibrarythatprovidesahigh-performanceembeddeddatabaseforkey/valuedata', 'Damballaparkour': 'DamballaParkour:MapReducelibraryforClojure', 'Couchbaseforest': 'CouchbaseForestDB:FastKey-ValueStorageEngineBasedonHierarchicalB+-TreeTrie', 'Spagobi': 'SpagoBI:opensourcebusinessintelligenceplatform', 'Akiban': 'Akiban:ToutedasSQLdatabasewithobjectstructuredstorage', 'Sherpa': 'YahooSherpa:hosted,distributedandgeographicallyreplicatedkey-valuecloudstorageplatform', 'Curator': 'ApacheCurator:JavalibariesforApacheZooKeeper', 'Pydoop': 'Pydoop:PythonMapReduceandHDFSAPIforHadoop', 'Datomic': 'Datomic:distributeddatabasedesignedtoenablescalable,flexibleandintelligentapplications', 'Brytlyt': 'Brytlyt:afullyenabledGPGPUdatabasewhichallowsforoffloadingofdatabaseoperationstoGeneralProcessingonGraphicsProcessorUnits.', 'Datalake': 'MicrosoftAzureDataLake:ahyperscalerepositoryforbigdataanalyticworkloads', 'Apollo': 'Apollo:ActiveMQsnextgenerationofmessaging', 'Metamarkersdruid': 'MetamarkersDruid:frameworkforreal-timeanalysisoflargedatasets', 'Caffe': 'Caffe:adeeplearningframeworkmadewithexpression,speed,andmodularityinmind.ItisdevelopedbytheBerkeleyVisionandLearningCente', '2lemetry': '2lemetry:PlatformforInternetofthings', 'Tarantool': 'Tarantool:anefficientNoSQLdatabaseandaLuaapplicationserver', 'Drools': 'Drools:aBusinessRulesManagementSystem(BRMS)solution', 'Fluentd': 'Fluentd:tooltocollecteventsandlogs', 'Oozie': 'ApacheOozie:workflowjobscheduler', 'Stardog': 'Stardog:graphdatabase:search,query,reasoning,andconstraintsinalightweight,pureJavasystem', 'Cephfilesystem': 'CephFilesystem:softwarestorageplatformdesigned', 'Everest': 'YahooEverest:multi-peta-bytedatabase/MPPderivedbyPostgreSQL', 'Grafana': 'Grafana:opensource,featurerichmetricsdashboardandgrapheditorforGraphite,InfluxDB&OpenTSDB', 'Bagel': 'ApacheSparkBagel:implementationofPregel,partofSpark', 'Apceranats': 'ApceraNATS:anopen-source,high-performance,lightweightcloudnativemessagingsystem', 'Webscale': 'WebScaleSQL:isacollaborationamongengineersfromseveralcompaniesthatfacesimilarchallengesinrunningMySQLatscale', 'Chart': 'Chart.js:opensourceHTML5Chartsvisualizations', 'Pivotalr': 'PivotalR:RonPivotalHD/HAWQandPostgreSQL', 'Palantiratlas': 'PalantirAtlasDB:amassivelyscalabledatastoreandtransactionallayerthatcanbeplacedontopofanykey-valuestoretogiveitACIDproperties', 'Chukwa': 'ApacheChukwa:datacollectionsystem', 'Jethrodata': 'JethroData:index-basedSQLengineforHadoop', 'Ambry': 'LinkedInAmbry:Distributedobjectstore', 'Parselystreamparse': 'ParselyStreamparse:streamparseletsyourunPythoncodeagainstreal-timestreamsofdata.ItalsointegratesPythonsmoothlywithApacheStorm.', 'Etsysahale': 'EtsySahale:VisualizingCascadingWorkflowsatEtsy', 'Zeromq': 'ZeroMQ:TheIntelligentTransportLayer', 'Cubism': 'Cubism:JavaScriptlibraryfortimeseriesvisualization', 'Exasolution': 'Exasolution:anin-memory,column-oriented,relationaldatabasemanagementsystem', 'Actianvector': 'ActianVector:column-orientedanalyticdatabase', 'Kamikaze': 'LinkedInKamikaze:utilitypackageforcompressingsortedintegerarrays', 'Intelgraphbuilder': 'IntelGraphBuilder:toolstoconstructlarge-scalegraphsontopofHadoop', 'Streams': 'IBMStreams:advancedanalyticplatformthatallowsuser-developedapplicationstoquicklyingest,analyzeandcorrelateinformationasitarrivesfromthousandsofreal-timesources', 'Simplequeueservice': 'AmazonSimpleQueueService:fast,reliable,scalable,fullymanagedqueueservice', 'Orient': 'OrientDB:documentandgraphdatabase', 'Lumos': 'LinkedinLumos:bridgefromOLTPtoOLAPforuseitonHadoop', 'Stratosphere': 'Stratosphere:generalpurposeclustercomputingframework', 'Firebase': 'GoogleFirebase:apowerfulAPItostoreandsyncdatainrealtime', 'Segment': 'SegmentSQL:TrackyourcustomerdatatoAmazonRedshift', 'Datasaltpangool': 'DatasaltPangool:alternativeMapReduceparadigm', 'Vahara': 'Vahara:MachinelearningandnaturallanguageprocessingwithApachePig', 'Shiny': 'Shiny:webapplicationframeworkforR', 'Autoscale': 'FacebookAutoscale:theloadbalancerwillconcentrateworkloadtoaserveruntilithasatleastamedium-levelworkload', 'Gospeed': 'LinkedInGoSpeed:providesRUMdataprocessing,visualization,monitoring,andanalysesdatadaily,hourly,oronanearreal-timebasis', 'S3mper': 'NetflixS3mper:librarythatprovidesanadditionallayerofconsistencycheckingontopofAmazonsS3indexthroughuseofaconsistent,secondaryindex', 'Sqrrl': 'Sqrrl:NoSQLdatabasesontopofApacheAccumulo', 'Wolframalpha': 'WolframAlpha:computationalknowledgeengine', 'Redissentinel': 'RedisSentinel:systemdesignedtohelpmanagingRedisinstances', 'Pub/sub': 'GoogleCloudPub/Sub:reliable,many-to-many,asynchronousmessaginghostedonGooglesinfrastructure', 'Gephi': 'Gephi:Anaward-winningopen-sourceplatformforvisualizingandmanipulatinglargegraphsandnetworkconnections', 'Scalingdata': 'ScalingData:tracingdatacenterproblemstorootcause,predictcapacityissues,identifyemergingfailuresandhighlightlatentthreats', 'Trill': 'MicrosoftTrill:ahigh-performancein-memoryincrementalanalyticsengine', 'Amplabsplash': 'AMPLabSplash:ageneralframeworkforparallelizingstochasticlearningalgorithmsonmulti-nodeclusters', 'Phoebus': 'Phoebus:frameworkforlargescalegraphprocessing', 'Deimos': 'Deimos:MesoscontainerizerhooksforDocker', 'Kibana': 'Kibana:visualizelogsandtime-stampeddata', 'Amplabshark': 'AMPLABShark:datawarehousesystemforSpark', 'Neo4j': 'Neo4j:graphdatabasewrittingentirelyinJava', 'Dockerswarm': 'DockerSwarm:nativeclusteringforDocker', 'Freeboard': 'Freeboard:opensourcereal-timedashboardbuilderforIOTandotherwebmashups', 'Streaming': 'ApacheSparkStreaming:frameworkforstreamprocessing,partofSpark', 'Thingworx': 'ThingWorx:Rapiddevelopmentandconnectionofintelligentsystems', 'Concurrentlingual': 'ConcurrentLingual:SQL-likequerylanguageforCascading', 'Datatorrentstram': 'DataTorrentStrAM:real-timeengineisdesignedtoenabledistributed,asynchronous,realtimein-memorybig-datacomputationsinasunblockedawayaspossible,withminimaloverheadandimpactonperformance', 'Datameer': 'Datameer:dataanalyticsapplicationforHadoopcombinesself-servicedataintegration,analyticsandvisualization', 'Bitlynsq': 'Bit.lyNSQ:realtimedistributedmessageprocessingatscale', 'Actianversant': 'ActianVersant:commercialobject-orienteddatabasemanagementsystems', 'Point': 'Clusterpoint:adatabasesoftwareforhigh-speedstorageandlarge-scaleprocessingofXMLandJSONdataonclustersofcommodityhardware', 'Pregel': 'GooglePregel:graphprocessingframework', 'Flink': 'ApacheFlink:high-performanceruntime,andautomaticprogramoptimization', 'Bookkeeper': 'ApacheBookKeeper:adistributedloggingservicecalledBookKeeperandadistributedpublish/subscribesystembuiltontopofBookKeepercalledHedwig', 'Parquet': 'Parquet:columnarstorageformatforHadoop', 'Cayley': 'GoogleCayley:open-sourcegraphdatabase', 'Mazerunnerforneo4j': 'MazerunnerforNeo4j:extendsaNeo4jgraphdatabasetorunscheduledbigdatagraphcomputealgorithmsatscalewithHDFSandApacheSpark.', 'Kestrel': 'Kestrel:distributedmessagequeuesystem', 'Voltagesecuredata': 'VoltageSecureData:dataprotectionframework', 'Twemcache': 'TwitterTwemcache:forkofMemcache', 'Drizzle': 'Drizzle:evolutionofMySQL6.0', 'Coreosfleet': 'CoreOSFleet:clustermanagementtoolfromCoreOS', 'Highcharts': 'Highcharts:simpleandflexiblechartingAPI', 'Director': 'ClouderaDirector:acomprehensivedatamanagementplatformwiththeflexibilityandpowertoevolvewithyourbusiness', 'Springxd': 'SpringXD:distributedandextensiblesystemfordataingestion,realtimeanalytics,batchprocessing,anddataexport', 'Prometheus': 'Prometheus:anopen-sourceservicemonitoringsystemandtimeseriesdatabase', 'Talend': 'Talend:unifiedopensourceenvironmentforYARN,Hadoop,HBASE,Hive,HCatalog&Pig', 'Distbelief': 'GoogleDistBelief:softwareframeworkthatcanutilizecomputingclusterswiththousandsofmachinestotrainlargemodels', 'Ebaykylin': 'eBayKylin:DistributedAnalyticsEnginefromeBayInc.thatprovidesSQLinterfaceandmulti-dimensionalanalysis(OLAP)onHadoopsupportingextremelylargedatasets', 'Jedoxpalo': 'JedoxPalo:customisableBusinessIntelligenceplatform', 'Crossfilter': 'Crossfilter:avaScriptlibraryforexploringlargemultivariatedatasetsinthebrowser.Workswellwithdc.jsandd3.js', 'Plotly': 'Plot.ly:Easy-to-usewebservicethatallowsforrapidcreationofcomplexcharts,fromheatmapstohistograms.UploaddatatocreateandstylechartswithPlotlysonlinespreadsheet.Forkothersplots.', 'Mapgraph': 'MapGraph:MassivelyParallelGraphprocessingonGPUs', 'Arvados': 'Arvados:Spinsawebofmicroservicesaroundunsuspectingsysadmins', 'Snowplow': 'Snowplow:enterprise-strengthwebandeventanalytics,poweredbyHadoop,Kinesis,RedshiftandPostgres', 'Concurrentpattern': 'ConcurrentPattern:machinelearninglibraryforCascading', 'Lingwater': 'SparklingWater:combineH2OÕsMachineLearningcapabilitieswiththepoweroftheSparkplatform', 'Phoenix': 'ApachePhoenix:SQLskinoverHBase', 'Evrything': 'Evrything:Makingproductssmart', 'Bigquery': 'GoogleBigQuery:frameworkforinteractiveanalysis,implementationofDremel', 'Raven': 'RavenDB:Atransactional,open-sourceDocumentDatabase', 'Nutch': 'ApacheNutch:opensourcewebcrawler', 'Recordbreaker': 'RecordBreaker:Automaticstructureforyourtext-formatteddata', 'Ankush': 'Ankush:Abigdataclustermanagementtoolthatcreatesandmanagesclustersofdifferenttechnologies.', 'Stratioviewer': 'StratioViewer:dashboardingtool', 'Galene': 'LinkedInGalene:searcharchitectureatLinkedIn', 'Lipstick': 'Lipstick:Pigworkflowvisualizationtool', 'Fbcunn': 'fbcunn:DeepLearningCUDAExtensionsfromFacebookAIResearch', 'Lavastorm': 'LavastormAnalytics:usedforauditanalytics,revenueassurance,fraudmanagement,andcustomerexperiencemanagement', 'Lucene': 'ApacheLucene:Searchenginelibrary', 'Zalonimica': 'ZaloniMica:self-servicedatadiscovery,curation,andgovernance', 'Develoop': 'Develoop:toolforprovisioning,managingandmonitoringApacheHadoop', 'Sparrow': 'Sparrow:schedulingplatform', 'Morphlines': 'ClouderaMorphlines:frameworkthathelpETLtoSolr,HBaseandHDFS', 'Flock': 'TwitterFlockDB:distribuitedgraphdatabase', 'Zookeeper': 'ApacheZookeeper:centralizedserviceforprocessmanagement', 'Drill': 'ApacheDrill:frameworkforinteractiveanalysis,inspiredbyDremel', 'Redash': 'Redash:open-sourceplatformtoqueryandvisualizedata', 'Dremel': 'GoogleDremel:frameworkforinteractiveanalysis,implementationofDremel', 'Featurefu': 'LinkedInFeatureFu:containsacollectionoflibrary/toolsforadvancedfeatureengineeringtoderivefeaturesontopofotherfeatures,orconvertalightweightedmodelintoafeature', 'Docker': 'Docker:anopenplatformfordevelopersandsysadminstobuild,ship,andrundistributedapplications', 'Hadapt': 'Hadapt:anativeimplementationofSQLfortheApacheHadoopopen-sourceproject', 'Simplestorageservice': 'AmazonSimpleStorageService:secure,durable,highly-scalableobjectstorage', 'Pivotalgemfirexd': 'PivotalGemFireXD:Low-latency,in-memory,distributedSQLdatastore.ProvidesSQLinterfacetoin-memorytabledata,persistableinHDFS', 'Beanstalkd': 'Beanstalkd:simple,fastworkqueue', 'Charts': 'GoogleCharts:simplechartingAPI', 'Tokutek': 'Tokutek:TokutekclaimstoimproveMongoDBperformance20x', 'Actianfor': 'ActianSQLforHadoop:highperformanceinteractiveSQLaccesstoallHadoopdata', 'Sqoop': 'ApacheSqoop:tooltotransferdatabetweenHadoopandastructureddatastore', 'Millwheel': 'GoogleMillWheel:faulttolerantstreamprocessingframework', 'Myria': 'Myria:scalableAnalytics-as-a-Serviceplatformbasedonrelationalalgebra', 'Slider': 'ApacheSlider:isaYARNapplicationtodeployexistingdistributedapplicationsonYARN', 'Ebayoink': 'eBayOink:RESTbasedinterfaceforPIGexecution', 'Myriad': 'Myriad:amesosframeworkdesignedforscalingYARNclustersonMesos.MyriadcanexpandorshrinkoneormoreYARNclustersinresponsetoeventsasperconfiguredrulesandpolicies.', 'Stratiocassandra': 'StratioCassandra:CassandraindexfunctionalityhasbeenextendedtoprovidenearrealtimesearchsuchasElasticSearchorSolr,includingfulltextsearchcapabilitiesandmultivariable,geospatialandbitemporalsearch', 'Spotfire': 'Spotfire:businessintelligenceplatform', 'Stratiocrossdata': 'StratioCrossdata:providesanunifiedwaytoaccesstomultipledatastores', 'Geotrellis': 'Geotrellis:geographicdataprocessingengineforhighperformanceapplications', 'Bigobject': 'BigObject:Real-timeComputingEngineDesignedforBigData', 'Aegisthus': 'NetflixAegisthus:BulkDataPipelineoutofCassandra.implementsareaderfortheSSTableformatandprovidesamap/reduceprogramtocreateacompactedsnapshotofthedatacontainedinacolumnfamily', 'Blink': 'BlinkDB:massivelyparallel,approximatequeryengine', 'Splicemachine': 'SpliceMachine:afull-featuredSQL-on-HadoopRDBMSwithACIDtransactions', 'Sumologic': 'SumoLogic:cloudbasedanalyzerformachine-generateddata.', 'Haystack': 'FacebookHaystack:objectstoragesystem', 'Actianp': 'ActianPSQL:ACID-compliantDBMSdevelopedbyPervasiveSoftware,optimizedforembeddinginapplications', 'Cubert': 'LinkedInCubert:afastandefficientbatchcomputationengineforcomplexanalysisandreportingofmassivedatasetsonHadoop', 'Seldon': 'Seldon:anopensourcepredictiveanalyticsplatformbaseduponSpark,KafkaandHadoop', 'Cudnn': 'cuDNN:GPU-acceleratedlibraryofprimitivesfordeepneuralnetworks', 'Sigmoidspork': 'SigmoidAnalyticsSpork:PigonApacheSpark', 'Heron': 'TwitterHeron:arealtime,distributed,fault-tolerantstreamprocessingenginefromTwitter', 'Camus': 'LinkedInCamus:KafkatoHDFSpipeline.ItisamapreducejobthatdoesdistributeddataloadsoutofKafka', 'Algolia': 'Algolia:HostedSearchAPIthatdeliversinstantandrelevantresultsfromthefirstkeystroke', 'Opents': 'OpenTSDB:distributedtimeseriesdatabaseontopofHBase', 'Stratiomanager': 'StratioManager:install,manageandmonitorallthetechnologystackrelatedtotheStratioPlatform', 'Hanoi': 'HanoiDB:ErlangLSMBTreeStorage', 'Simple': 'AmazonSimpleDB:ahighlyavailableandflexiblenon-relationaldatastorethatoffloadstheworkofdatabaseadministration', 'Sidekiq': 'Sidekiq:Simple,efficientbackgroundprocessingforRuby', 'Rainstor': 'RainstorDB:databaseforstoringpetabyte-scalevolumesofstructuredandsemi-structureddata', 'Fatcache': 'TwitterFatcache:key/valuecacheforflashstorage', 'Nokiadisco': 'NokiaDisco:MapReduceframeworkdevelopedbyNokia', 'Beegfs': 'BeeGFS:formerlyFhGFS,paralleldistributedfilesystem', 'Crunch': 'ApacheCrunch:asimpleJavaAPIfortaskslikejoininganddataaggregationthataretedioustoimplementonplainMapReduce', 'Sawmill': 'Sawmill:extensivelogprocessingandreportingfeatures', 'Gistoolsfor': 'GISToolsforHadoop:BigDataSpatialAnalyticsfortheHadoopFramework', 'Darner': 'Darner:simple,lightweightmessagequeue', 'Kairos': 'Kairos:TimeseriesdatastorageinRedis,Mongo,SQLandCassandra', 'Eventhub': 'Eventhub:opensourceeventanalyticsplatform', 'Reborn': 'RebornDB:Distributeddatabasefullycompatiblewithredisprotocol', 'Crossroadsi/o': 'CrossroadsI/O:libraryforbuildingscalableandhighperformancedistributedapplications', 'Snowball': 'AmazonSnowball:apetabyte-scaledatatransportsolutionthatusessecureappliancestotransferlargeamountsofdataintoandoutofAWS', 'Sensei': 'SenseiDB:distributed,realtime,semi-structureddatabase', 'Bigchain': 'BigchainDB:Thescalableblockchaindatabase.', 'Fblearnerflow': 'FacebookFBLearnerFlow:providesinnovativefunctionality,likeautomaticgenerationofUIexperiencesfrompipelinedefinitionsandautomaticparallelizationofPythoncodeusingfutures', 'Flumejava': 'GoogleFlumeJava:Easy,EfficientData-ParallelPipelines.BaseofGoogleDataflow', 'Impala': 'ClouderaImpala:frameworkforinteractiveanalysis,InspiredbyDremel', 'Zoomdata': 'Zoomdata:BigDataAnalytics', 'Omega': 'GoogleOmega:jobschedulingandmonitoringsystem', 'Knoxgateway': 'ApacheKnoxGateway:singlepointofsecureaccessforHadoopclusters', 'Datapine': 'Datapine:self-servicebusinessintelligencetoolinthecloud', 'Graphistry': 'Graphistry:runningonGPUsandturnsstaticdesignsintointeractivetoolsusingclient/cloudGPUinfrastructureandGPU-acceleratedlanguageslikeSuperconductor', 'Foundation': 'FoundationDB:distributeddatabase,inspiredby\\xa0F1', 'Hashicorpnomad': 'HashiCorpNomad:aDistributed,HighlyAvailable,Datacenter-AwareScheduler', 'Tableau': 'Tableau:businessintelligenceplatform', 'Titan': 'Titan:distributedgraphdatabase,builtoverCassandra', 'Biginsights': 'IBMBigInsights:dataprocessing,warehousingandanalytics', 'Scaleouthserver': 'ScaleOuthServer:fast,scalablein-memorydatagridforHadoop', 'Pumabenchmarking': 'PUMABenchmarking:benchmarksuiteforMapReduceapplications', 'Dynomite': 'NetflixDynomite:thinDynamo-basedreplicationforcacheddata', 'Quicksight': 'AmazonQuickSight:BusinessIntelligenceforBigData', 'Streamblaze': 'SQLStreamBlaze:streamprocessingplatform', 'Mesos': 'ApacheMesos:clustermanager', 'Ribbon': 'NetflixRibbon:aInterProcessCommunication(remoteprocedurecalls)librarywithbuiltinsoftwareloadbalancers.TheprimaryusagemodelinvolvesRESTcallswithvariousserializationschemesupport', 'Amplabsampleclean': 'AMPLabSampleClean:scalabletechniquesfordatacleaningandstatisticalinferenceondirtydata', 'Megastore': 'GoogleMegastore:scalable,highlyavailablestorage', 'Bigtop': 'ApacheBigtop:systemdeploymentframeworkfortheHadoopecosystem', 'Metanautixquest': 'MetanautixQuest:datacomputeengine', 'Pivotalhawq': 'PivotalHAWQ:SQL-likedatawarehousesystemfor\\xa0Hadoop', 'Akkatoolkit': 'AkkaToolkit:runtimefordistributed,andfaulttolerantevent-drivenapplicationsontheJVM', 'Ranger': 'ApacheRanger:frameworktoenable,monitorandmanagecomprehensivedatasecurityacrosstheHadoopplatform(formerlycalledApacheArgus)', 'Eventhubs': 'AzureEventHubs:ahighlyscalablepublish-subscribeeventingestor', 'Infinitegraph': 'InfiniteGraph:distributedgraphdatabase', 'Tessera': 'Tessera:EnvironmentforDeepAnalysisofLargeComplexData', 'Caffeine': 'GoogleCaffeine:continuousindexingsystem', 'Stratiodecision': 'StratioDecision:theunionofareal-timemessagingbuswithacomplexeventprocessingengineusingSparkStreaming', 'Gobblin': 'LinkedInGobblin:aframeworkforSolvingBigDataIngestionProblem', 'Cassandra': 'ApacheCassandra:column-orienteddistribuiteddatastore,inspiredby\\xa0BigTable', 'Hbase': 'ApacheHBase:column-orienteddistribuiteddatastore,inspiredbyBigTable', 'Influx': 'InfluxDB:distributedtimeseriesdatabase', 'Kinesis': 'AmazonKinesis:real-timeprocessingofstreamingdataatmassivescale', 'Twemproxy': 'Twemproxy:Afast,light-weightproxyformemcachedandredis', 'Spanner': 'GoogleSpanner:globallydistributedsemi-relationaldatabase', 'Memgraph': 'MemGraph:cyphercompatibile,high-performancein-memorytransactionalandreal-timeanalyticsgraphdatabase', 'Blaze': 'Blaze:Pythonusershigh-levelaccesstoefficientcomputationoninconvenientlylargedata', 'Graphengine': 'MicrosoftGraphEngine:adistributed,in-memory,largegraphprocessingengine,underpinnedbyastrongly-typedRAMstoreandageneralcomputationengine', 'Whirr': 'ApacheWhirr:setoflibrariesforrunningcloudservices', 'Document': 'MicrosoftDocumentDB:fully-managed,highly-scalable,NoSQLdocumentdatabaseservice', 'Gizzard': 'TwitterGizzard:aflexibleshardingframeworkforcreatingeventually-consistentdistributeddatastores', 'Zipkin': 'TwitterZipkin:distributedtracingsystemthathelpsusgathertimingdataforallthedisparateservicesatTwitter', 'Percolator': 'GooglePercolator:continuousindexingsystem', 'Mcdipper': 'FacebookMcDipper:key/valuecacheforflashstorage', 'Graphlabpowergraph': 'GraphLabPowerGraph:acoreC++GraphLabAPIandacollectionofhigh-performancemachinelearninganddataminingtoolkitsbuiltontopoftheGraphLabAPI', 'Storm': 'ApacheStorm:frameworkforstreamprocessingbyTwitteralsoonYARN', 'Jumbune': 'Jumbune:Jumbuneisanopen-sourceproductbuiltforanalyzingHadoopclusterandMapReducejobs.', 'Hipilibrary': 'HIPILibrary:APIforperformingimageprocessingtasksonHadoopsMapReduce', 'Cytoscape': 'Cytoscape:JavaScriptlibraryforvisualizingcomplexnetworks', 'Nupic': 'nupic:NumentaPlatformforIntelligentComputing:abrain-inspiredmachineintelligenceplatform,andbiologicallyaccurateneuralnetworkbasedoncorticallearningalgorithms', 'Openaigym': 'OpenAIGym:atoolkitfordevelopingandcomparingreinforcementlearningalgorithms', 'Pinball': 'PinterestPinball:customizableplatformforcreatingworkflowmanagers', 'Stratioingestion': 'StratioIngestion:ApacheFlumewithsteroids', 'Sentry': 'ApacheSentry:securitymodulefordatastoredinHadoop', 'Ambari': 'ApacheAmbari:operationalframeworkforHadoopmangement', 'Pneuralnet': 'MLPNeuralNet:FastmultilayerperceptronneuralnetworklibraryforiOSandMacOSX', 'Hparser': 'HParser:dataparsingtransformationenvironmentoptimizedforHadoop', 'Cascalog': 'Cascalog:dataprocessingandqueryinglibrary', 'Rabbitmq': 'RabbitMQ:Robustmessagingforapplications', 'Storehaus': 'Storehaus:librarytoworkwithasynchronouskeyvaluestores,byTwitter', 'Pinlater': 'PinterestPinlater:asynchronousjobexecutionsystem', 'Amplabsimr': 'AMPLabSIMR:runSparkonHadoopMapReducev1', 'Sf1rsearchengine': 'SF1RSearchEngine:distributedsearchenginewritteninc++', 'Surus': 'NetflixSurus:acollectionoftoolsforanalysisinPigandHive', 'Arango': 'ArangoDB:multimodeldistribuiteddatabase', 'Etsystatsd': 'EtsyStatsD:simpledaemonforeasystatsaggregation', 'Thunder': 'Thunder:Large-scaleanalysisofneuraldata', 'Oraclenodatabase': 'OracleNoSQLDatabase:distributedkey-valuedatabasebyOracleCorporation', 'Containerengine': 'GoogleContainerEngine:RunDockercontainersonGoogleCloudPlatform,poweredbyKubernetes', 'Graphite': 'Graphite:scalableRealtimeGraphing', 'Priam': 'NetflixPriam:Co-Processforbackup/recovery,TokenManagement,andCentralizedConfigurationmanagementforCassandra', 'Storsimple': 'MicrosoftStorSimple:auniquehybridcloudstoragesolutionthatlowerscostsandimprovesdataprotection', 'Tigon': 'Tigon:adistributedframeworkbuiltonApacheHadoopTMandApacheHBaseTMforreal-time,high-throughput,low-latencydataprocessingandanalyticsapplications', 'Tephra': 'Tephra:TransactionsforHBase', 'Peregrine': 'FacebookPeregrine:MapReduceframework', 'Lambda': 'AmazonLambda:acomputeservicethatrunsyourcodeinresponsetoeventsandautomaticallymanagesthecomputeresourcesforyou', 'Summingbird': 'TwitterSummingbird:StreamingMapReducewithScaldingandStorm,byTwitter', 'Hbasecoprocessor': 'HBaseCoprocessor:implementationof\\xa0Percolator,partof\\xa0HBase', 'Hazelcast': 'Hazelcast:In-MemoryDataGrid', 'Dynamo': 'AmazonDynamoDB:distributedkey/valuestore,implementationof\\xa0Dynamopaper', 'Gridgain': 'GridGain:GGFS,Hadoopcompliantin-memoryfilesystem', 'Infovore': 'Infovore:RDF-centricMap/Reduceframework', 'Ohmdatac5': 'OhmDataC5:improvedversionofHBase', 'Amplabvelox': 'AMPLabVelox:adatamanagementsystemforfacilitatingthenextstepsinreal-world,large-scaleanalyticspipelines', 'Recline': 'Recline:simplebutpowerfullibraryforbuildingdataapplicationsinpureJavascriptandHTML', 'Domino': 'Domino:Run,scale,share,anddeploymodelsÑwithoutanyinfrastructure.', 'Tachyon': 'Tachyon:reliablefilesharingatmemoryspeedacrossclusterframeworks', 'Pivotalgreenplum': 'PivotalGreenplum:purpose-built,dedicatedanalyticdatawarehouse', 'Pumba': 'Pumba:ChaostestingtoolforDocker', 'Elephant': 'ElephantDB:DistributeddatabasespecializedinexportingdatafromHadoop', 'Mantis': 'NetflixMantis:EventStreamProcessingSystem', 'Logstash': 'Logstash:atoolformanagingeventsandlogs', 'Giraph': 'ApacheGiraph:implementationofPregel,basedonHadoop', 'Infini': 'InfiniDB:isaccessedthroughaMySQLinterfaceandusemassiveparallelprocessingtoparallelizequeries', 'Ironmq': 'IronMQ:easy-to-usehighlyavailablemessagequeuingservice', 'Mcrouter': 'FacebookMcrouter:amemcachedprotocolrouterforscalingmemcacheddeployments', 'Terrastore': 'Terrastore:amoderndocumentstorewhichprovidesadvancedscalabilityandelasticityfeatureswithoutsacrificingconsistency', 'Karaf': 'ApacheKaraf:OSGiruntimethatrunsontopofanyOSGiframework', 'Mahout': 'ApacheMahout:machinelearninglibraryforHadoop', 'Getstreamstreamframework': 'GetStreamStreamFramework:aPythonlibrary,whichallowsyoutobuildnewsfeedandnotificationsystemsusingCassandraand/orRedis', 'Enigmaio': 'Enigma.io:Freemiumrobustwebapplicationforexploring,filtering,analyzing,searchingandexportingmassivedatasetsscrapedfromacrosstheWeb', 'Lustrefilesystem': 'Lustrefilesystem:high-performancedistributedfilesystem', 'Matplotlib': 'Matplotlib:plottingwithPython', 'Storage': 'GoogleCloudStorage:durableandhighlyavailableobjectstorage', 'Krati': 'LinkedInKrati:isasimplepersistentdatastorewithverylowlatencyandhighthroughput', 'Etsyconjecture': 'EtsyConjecture:scalableMachineLearninginScalding', 'Chubby': 'GoogleChubby:alockserviceforloosely-coupleddistributedsystems', 'Stratioexplorer': 'StratioExplorer:anInteractiveWebinterpretertoApacheCrossdata,StratioIngestion,StratioDecision,Markdown,ApacheSpark,ApacheSpark-SQLandcommandShell', 'Hindex': 'hIndex:SecondaryIndexforHBase', 'Hypergraph': 'HyperGraphDB:generalpurpose,open-sourcedatastoragemechanismbasedonapowerfulknowledgemanagementformalismknownasdirectedhypergraphs', 'Velox': 'Velox:asystemforservingmachinelearningpredictions', 'Sphinxsearchserver': 'SphinxSearchServer:fulltextsearchengine', 'Optiq': 'ApacheOptiq:frameworkthatallowsefficienttranslationofqueriesinvolvingheterogeneousandfederateddata', 'Yhatscienceops': 'YHatScienceOps:platformfordeploying,managing,andscalingpredictivemodelsinproductionapplications', 'Pinot': 'LinkedInPinot:adistributedsystemthatsupportscolumnarindexeswiththeabilitytoaddnewtypesofindexes', 'Buildoop': 'Buildoop:SimilartoApacheBigTopbasedonGroovylanguage', 'Mesosaurus': 'Mesosaurus:Mesostaskloadsimulatorframeworkfor(clusterandMesos)performanceanalysis', 'Microstrategy': 'Microstrategy:softwareplatformsforbusinessintelligence,mobileintelligence,andnetworkapplications', 'Nextflow': 'Nextflow:Datafloworientedtoolkitforparallelanddistributedcomputationalpipelines', 'Databus': 'LinkedInDatabus:streamofchangecaptureeventsforadatabase', 'Actianingres': 'ActianIngres:commerciallysupported,open-sourceSQLrelationaldatabasemanagementsystem', 'Tumblrgenesis': 'TumblrGenesis:atoolfordatacenterautomation', 'Keystone': 'KeystoneML:Simplifyingrobustend-to-endmachinelearningonApacheSpark', 'Gearpump': 'GearPump:alightweightreal-timebigdatastreamingengine', 'Teradatadatabase': 'TeradataDatabase:completerelationaldatabasemanagementsystem', 'Stado': 'Stado:opensourceMPPdatabasesystemsolelytargetedatdatawarehousinganddatamartapplications', 'Youtubevitess': 'YoutubeVitess:providesserversandtoolswhichfacilitatescalingofMySQLdatabasesforlargescalewebservices', 'Mpich': 'MPICH:highperformanceandwidelyportableimplementationoftheMessagePassingInterface(MPI)standard', 'Pentaho': 'Pentaho:businessintelligenceplatform', 'Pinalytics': 'Pinalytics:Pinterestâ\\x80\\x99sdataanalyticsengine', 'Stratiota': 'StratioSparkta:realtimemonitoring', 'Restmq': 'RestMQ:messagequeuewhichusesHTTPastransport,JSONtoformataminimalistprotocolandisorganizedasRESTresources', 'Jumbo': 'jumboDB:documentorienteddatastoreoverHadoop', 'Sigma': 'Sigma.js:JavaScriptlibrarydedicatedtographdrawing', 'Helix': 'ApacheHelix:clustermanagementframework', 'Catalyst': 'SparkCatalyst:isaQueryOptimizationFrameworkforSparkandShark', 'Chartist': 'Chartist.js:anotheropensourceHTML5Chartsvisualization', 'Tibcoenterprisemessageservice': 'TIBCOEnterpriseMessageService:standards-basedmessagingmiddleware', 'Scuba': 'FacebookScuba:distributedin-memorydatastore', 'Sibyl': 'GoogleSibyl:SystemforLargeScaleMachineLearningatGoogle', 'Mapreduce': 'GoogleMapReduce:mapreduceframework', 'Twill': 'ApacheTwill:abstractionoverYARNthatreducesthecomplexityofdevelopingdistributedapplications', 'Kairosdb': 'Kairosdb:similartoOpenTSDBbutallowsforCassandra', 'Orleans': 'MicrosoftOrleans:astraightforwardapproachtobuildingdistributedhigh-scalecomputingapplications', 'Rocket': 'Rocket:analternativetotheDockerruntime,designedforserverenvironmentswiththemostrigoroussecurityandproductionrequirements', 'Sanddance': 'MicrosoftSandDance:visuallyexploredatasetstofindstoriesandextractinsights', 'Handlersocket': 'HandlerSocket:NoSQLpluginforMySQL/MariaDB', 'Fnordmetricchart': 'FnordMetricChartSQL:allowsyoutowriteSQLqueriesthatreturnchartsinsteadoftables.ThechartsarerenderedasSVGvectorgraphics.', 'Marathon': 'Marathon:Mesosframeworkforlong-runningservices', 'Watson': 'IBMWatson:cognitivecomputingsystem', 'Samza': 'ApacheSamza:streamprocessingframework,basedonKaflaandYARN', 'Stinger': 'Stinger:interactivequeryforHive', 'Eventstore': 'EventStore:distributedtimeseriesdatabase', 'Vowpalwabbit': 'VowpalWabbit:learningsystemsponsoredbyMicrosoftandYahoo!', 'Redis': 'Redis:inmemorykeyvaluedatastore', 'Haeinsa': 'Haeinsa:linearlyscalablemulti-row,multi-tabletransactionlibraryforHBasebasedonPercolator', 'Qubole': 'Qubole:auto-scalingHadoopcluster,built-indataconnectors', 'Berkeleyswimbenchmark': 'BerkeleySWIMBenchmark:real-worldbigdataworkloadbenchmark', 'Distributedr': 'DistributedR:scalablehigh-performanceplatformfortheRlanguage', 'Hannibal': 'Hannibal:HannibalistooltohelpmonitorandmaintainHBase-Clustersthatareconfiguredformanualsplitting.', 'Benchmarking': 'ApacheHadoopBenchmarking:micro-benchmarksfortestingHadoopperformances', 'Projectorleans': 'MicrosoftProjectOrleans:aframeworkthatprovidesastraightforwardapproachtobuildingdistributedhigh-scalecomputingapplications', 'Hortonworkshoya': 'HortonworksHOYA:applicationthatcandeployHBaseclusteronYARN', 'Spice': 'AmazonSPICE:Super-fastParallelIn-memoryCalculationEngine', 'Adabas': 'Adabas:ADABASwasNoSQLfromatimewhentherewasnoSQL', 'Prism': 'FacebookPrism:multidatacentersreplicationsystem', 'Ec2containerservice': 'AmazonEC2ContainerService:ahighlyscalable,highperformancecontainermanagementservicethatsupportsDockercontainers', 'Gangliamonitoring': 'GangliaMonitoringSystem:scalabledistributedmonitoringsystemforhigh-performancecomputingsystemssuchasclustersandGrids', 'Extreme': 'eXtremeDB:in-memorydatabasecombinesexceptionalperformance,reliabilityanddeveloperefficiencyinaprovenreal-timeembeddeddatabaseengine', 'Trafodion': 'Trafodion:enterprise-classSQL-on-HBasesolutiontargetingbigdatatransactionaloroperationalworkloads', 'Elephantbird': 'TwitterElephantBird:librariesforworkingwithLZOP-compresseddata', 'Redhatglusterfs': 'RedHatGlusterFS:scale-outnetwork-attachedstoragefilesystem', 'Brooklyn': 'Brooklyn:librarythatsimplifiesapplicationdeploymentandmanagement', 'Tibcoactivespaces': 'TIBCOActiveSpaces:in-memorydatagrid', 'Succinct': 'SuccinctSpark:EnablingQueriesonCompressedData', 'Marklogic': 'MarkLogic:Schema-agnosticEnterpriseNoSQLdatabasetechnology', 'Photon': 'GooglePhoton:geographicallydistributedsystemforjoiningmultiplecontinuouslyflowingstreamsofdatainreal-timewithhighscalabilityandlowlatency', 'Tempoiq': 'TempoIQ:Cloud-basedsensoranalytics', 'Hornetq': 'HornetQ:opensourceprojecttobuildamulti-protocol,embeddable,veryhighperformance,clustered,asynchronousmessagingsystem', 'Aurora': 'AmazonAurora:aMySQL-compatible,relationaldatabaseenginethatcombinesthespeedandavailabilityofhigh-endcommercialdatabaseswiththesimplicityandcost-effectivenessofopensourcedatabases', 'Tokiocabinet': 'TokioCabinet:alibraryofroutinesformanagingadatabase', 'Gearman': 'Gearman:JobServer', 'Jaspersoft': 'Jaspersoft:powerfulbusinessintelligencesuite', 'Airbnbairflow': 'AirBnBAirflow:AirFlowisasystemtoprogrammaticallyauthor,scheduleandmonitordatapipelines', 'Inviso': 'NetflixInviso:performancefocusedBigDatatool', 'Treode': 'TreodeDB:key-valuestorethatsreplicatedandshardedandprovidesatomicmultirowwrites', 'Ignite': 'ApacheIgnite:high-performance,integratedanddistributedin-memoryplatformforcomputingandtransactingonlarge-scaledatasetsinreal-time', 'Whiteelephant': 'LinkedInWhiteElephant:logaggregatoranddashboard', 'Datasaltsplout': 'DatasaltSploutSQL:fullSQLqueryengineforbigdatasets', 'Oracledatabase': 'OracleDatabase:object-relationaldatabasemanagementsystem', 'Hamster': 'HamsterDB:transactionalkey-valuedatabase', 'Packetpig': 'PacketPig:OpenSourceBigDataSecurityAnalytics', 'Vibedatastream': 'VibeDataStream:streamingdatacollectionforreal-timeBigDataanalytics', 'Corona': 'FacebookCorona:Hadoopenhancementwhichremovessinglepointoffailure', 'Norbert': 'LinkedinNorbert:clustermanager', 'Elasticfile': 'AmazonElasticFileSystem:filestorageserviceforAmazonElasticComputeCloud(AmazonEC2)instances', 'Activepivot': 'ActivePivot:JavaIn-MemoryOLAPcubestoredincolumns,withclearlydecoupledpre/postprocessing', 'Hypertable': 'Hypertable:column-orienteddistribuiteddatastore,inspiredby\\xa0BigTable', 'Activemq': 'ActiveMQ:opensourcemessagingandIntegrationPatternsserver', 'Unqlite': 'UnQLite:ain-processsoftwarelibrarywhichimplementsaself-contained,serverless,zero-configuration,transactionalNoSQLdatabaseengine', 'Seqpig': 'SeqPig:Simpleandscalablescriptingforlargesequencingdataset(ex:bioinfomation)inHadoop', 'Flume': 'ApacheFlume:servicetomanagelargeamountoflogdata', 'Deepcl': 'DeepCL:OpenCLlibrarytotraindeepconvolutionalneuralnetworks', 'Convnetjs': 'convnetjs:DeepLearninginJavascript.TrainConvolutionalNeuralNetworks(orordinaryones)inyourbrowser', 'Cheetah': 'Cheetah:HighPerformance,CustomDataWarehouseonTopofMapReduce', 'Stratiostreaming': 'StratioStreaming:theunionofareal-timemessagingbuswithacomplexeventprocessingengineusingSparkStreaming', 'Datafu': 'ApacheDataFu:collectionofuser-definedfunctionsfor\\xa0HadoopandPigdevelopedbyLinkedIn', 'Azkaban': 'LinkedinAzkaban:batchworkflowjobscheduler', 'Boxtron': 'BoxTron:proxytomemcachedservers', 'Chronos': 'Chronos:distributedandfault-tolerantscheduler', 'Thrift': 'ApacheThrift:frameworktobuildbinaryprotocols', 'Addthishydra': 'AddThisHydra:distributeddataprocessingandstoragesystemoriginallydevelopedatAddThis', 'Espresso': 'LinkedInEspresso:horizontallyscalabledocument-orientedNoSQLdatastore', 'Intelhibench': 'IntelHiBench:aHadoopbenchmarksuite', 'Graphlabdato': 'GraphLabDato:fast,scalableengineofGraphLabCreate,aPythonlibrary', 'Datastore': 'GoogleCloudDatastore:isafullymanaged,schemalessdatabaseforstoringnon-relationaldataoverBigTable', 'Teradataaster': 'TeradataAster:BigDataAnalytics', 'Rocks': 'RocksDB:embeddablepersistentkey-valuestoreforfaststoragebasedonLevelDB', 'Decider': 'Decider:FlexibleandExtensibleMachineLearninginRuby', 'Lilyhbaseindexer': 'LilyHBaseIndexer:quicklyandeasilysearchforanycontentstoredinHBase', 'Hyperdex': 'HyperDex:nextgenerationkey-valuestore', 'Physics': 'CloudPhysics:collectoperationalmetadatafromyourvirtualizedinfrastructure,thencorrelateandanalyzeittoexposeoperationalhazardsandwastethatposeathreattoyourdatacenterperformance,efficiencyanduptime', 'Quantcastfileqfs': 'QuantcastFileSystemQFS:open-sourcedistributedfilesystem', 'Luigi': 'SpotifyLuigi:aPythonpackageforbuildingcomplexpipelinesofbatchjobs.Ithandlesdependencyresolution,workflowmanagement,visualization,handlingfailures,commandlineintegration,andmuchmore', 'Gridmix3': 'YahooGridmix3:HadoopclusterbenchmarkingfromYahooengineerteam', 'Falcon': 'ApacheFalcon:datamanagementframework', 'Spatial': 'SpatialHadoop:SpatialHadoopisaMapReduceextensiontoApacheHadoopdesignedspeciallytoworkwithspatialdata.', 'Staash': 'NetflixSTAASH:language-agnosticaswellasstorage-agnosticwebinterfaceforstoringdataintopersistentstoragesystems', 'Bayes': 'BayesDB:statisticorientedSQLdatabase', 'Dataflow': 'GoogleDataflow:createdatapipelinestohelpthemæingest,transformandanalyzedata', 'Madlib': 'MADlib:data-processinglibraryofanRDBMStoanalyzedata', 'Graphx': 'GraphX:resilientDistributedGraphSystemonSpark', 'Kafka': 'ApacheKafka:distributedpublish-subscribemessagingsystem', 'Arbor': 'Arbor:graphvisualizationlibraryusingwebworkersandjQuery', 'Memcached': 'FacebookMemcached:forkofMemcache'}\n"
     ]
    }
   ],
   "source": [
    "tags = ['Amazon','Apache','Adobe', 'Facebook','Twitter','Ebay', 'Netflix', 'LinkedIn', 'Spotify', \"Cluster\", 'Hive', 'Analytics',\n",
    "        'Google', 'Hadoop','Linkedin', 'Pinterest', 'Cloudera', 'Cloud', 'Eclipse','Microsoft', 'SQL', \n",
    "        'DB', '.js', 'Yahoo', 'Azure', 'ML', 'Machine Learning', 'IBM', \"SAP\", \"System\", \"Spark\",'.']\n",
    "\n",
    "def reduce(function, iterable, initializer=None):\n",
    "    it = iter(iterable)\n",
    "    if initializer is None:\n",
    "        try:\n",
    "            initializer = next(it)\n",
    "        except StopIteration:\n",
    "            raise TypeError('reduce() of empty sequence with no initial value')\n",
    "    accum_value = initializer\n",
    "    for x in it:\n",
    "        accum_value = function(accum_value, x)\n",
    "    return accum_value\n",
    "\n",
    "\n",
    "corrected_dict = { reduce(lambda a,b: a.replace(b, '').replace('-',' '), tags, x) : dict_big_data_techno[x] for x in dict_big_data_techno.keys() }\n",
    "\n",
    "\n",
    "final_dict_big_data_techno = { k.capitalize():v for k,v in corrected_dict.items() if \" \" not in k and len(k) > 4 }\n",
    "print(final_dict_big_data_techno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(final_dict_big_data_techno, orient  = 'index')\n",
    "df.reset_index(inplace = True)\n",
    "df.columns = ['name', 'desc']\n",
    "df.to_csv(\"../Scraping/big_data_database.csv\",encoding = \"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation pour Flask\n",
    "\n",
    "Ici on prépare le bout de code qui servira pour Flask :\n",
    "- l'import des fichiers avec les noms de Pokémons et de Big data\n",
    "- la création d'un dictionnaire identifiant les pokemons comme des pokemons et les technos big data comme telles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import collections\n",
    "import pandas\n",
    "\n",
    "df_pokemon = pd.read_csv(\"./pokemons_database.csv\")\n",
    "df_big_data = pd.read_csv(\"./big_data_database.csv\")\n",
    "\n",
    "dict_pokemons = db_pokemon[['name','desc']].set_index('name').to_dict()['desc']\n",
    "dict_big_data = df_big_data[['name', 'desc']].set_index('name').to_dict()['desc']\n",
    "\n",
    "dict_global  = collections.defaultdict()\n",
    "\n",
    "### On crée le dictionnaire qui regroupe les pokemons et les techno big data\n",
    "\n",
    "for pokemon in dict_pokemons.keys() :  \n",
    "    dict_global[pokemon] = \"Pokemon\"\n",
    "\n",
    "for techno in dict_big_data.keys() :  \n",
    "    dict_global[techno] = \"Big Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on choisit 10 éléments Pokemon et Big Data pour réaliser le questionnaire.\n",
    "\n",
    "Grâce à random.seed() on choisit aléatoirement parmi les noms disponibles mais le résultat sera toujours le même. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flaaffy Pokemon\n",
      "Myria Big Data\n",
      "Featurefu Big Data\n",
      "Sceptile Pokemon\n",
      "Akiban Big Data\n",
      "Mime-jr Pokemon\n",
      "Bookkeeper Big Data\n",
      "Hparser Big Data\n",
      "Chesnaught Pokemon\n",
      "Velox Big Data\n",
      "Lanturn Pokemon\n"
     ]
    }
   ],
   "source": [
    "r = random.Random(100)\n",
    "\n",
    "# on choisit 10 éléments dans la liste globale des pokemons et des techno big data    \n",
    "global_list = list(dict_pokemons.keys())+list(dict_big_data.keys())\n",
    "liste_choice =[]\n",
    "\n",
    "for x in range(0,11) : \n",
    "    liste_choice.append(r.choice(global_list))\n",
    "for item in liste_choice : \n",
    "    print(item, dict_global[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "liste_choice = ['Flaaffy',\n",
    " 'Myria',\n",
    " 'Featurefu',\n",
    " 'Sceptile',\n",
    " 'Akiban',\n",
    " 'Mime-jr',\n",
    " 'Bookkeeper',\n",
    " 'Hparser',\n",
    " 'Chesnaught',\n",
    " 'Velox',\n",
    " 'Lanturn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'3': {'options': ['Pokemon', 'Big Data'], 'question': 'Featurefu : Pokemon or Big Data ?', 'answer': 'Big Data'}, '4': {'options': ['Pokemon', 'Big Data'], 'question': 'Sceptile : Pokemon or Big Data ?', 'answer': 'Pokemon'}, '6': {'options': ['Pokemon', 'Big Data'], 'question': 'Mime-jr : Pokemon or Big Data ?', 'answer': 'Pokemon'}, '1': {'options': ['Pokemon', 'Big Data'], 'question': 'Flaaffy : Pokemon or Big Data ?', 'answer': 'Pokemon'}, '7': {'options': ['Pokemon', 'Big Data'], 'question': 'Bookkeeper : Pokemon or Big Data ?', 'answer': 'Big Data'}, '2': {'options': ['Pokemon', 'Big Data'], 'question': 'Myria : Pokemon or Big Data ?', 'answer': 'Big Data'}, '8': {'options': ['Pokemon', 'Big Data'], 'question': 'Hparser : Pokemon or Big Data ?', 'answer': 'Big Data'}, '9': {'options': ['Pokemon', 'Big Data'], 'question': 'Chesnaught : Pokemon or Big Data ?', 'answer': 'Pokemon'}, '5': {'options': ['Pokemon', 'Big Data'], 'question': 'Akiban : Pokemon or Big Data ?', 'answer': 'Big Data'}, '10': {'options': ['Pokemon', 'Big Data'], 'question': 'Velox : Pokemon or Big Data ?', 'answer': 'Big Data'}})\n"
     ]
    }
   ],
   "source": [
    "questions = collections.defaultdict()    \n",
    "for x in range(1,11) : \n",
    "    questions[str(x)] = {\"question\" : \"{} : Pokemon or Big Data ?\".format(liste_choice[x-1]), \"options\" :[\"Pokemon\", \"Big Data\"], \"answer\" : dict_global[liste_choice[x-1]]}\n",
    "    \n",
    "print(questions)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "0abec88b888cdd7d217e1a076e8ae590b3c15563fd1ea1e4de0524b60dc4d004"
  },
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
